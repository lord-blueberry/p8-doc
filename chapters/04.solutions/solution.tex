\section{Eliminating the Major Cycle}\label{killmajor}
Although Compressed Sensing algorithms produce higher quality reconstructions, the CLEAN algorithm so far has lower runtime costs. Both use a version of the Major Cycle architecture for efficient transformation of Visibilities to image space, and to correct the effects of wide field of view imaging. This project explores different architectures for Compressed Sensing, with the goal to decrease the runtime costs in the context of MeerKAT.

To our knowledge, there is little published research in this area. Many Compressed Sensing reconstructions use the Major Cycle architecture\cite{girard2015sparse}\cite{mcewen2011compressed}\cite{pratley2018fast}\cite{dabbech2018cygnus} and do not discuss alternatives. Pratley et al\cite{pratley2017robust} optimized the non-uniform FFT in the context of Compressed Sensing reconstructions. 

Similar works with: Hardy et al.\cite{hardy2013direct} removed the non-uniform FFT approximation with the explicit Fourier Transform Matrix and reported an easy framework for distribution
\cite{dabbech2017wEffect} is the recent work which introduces 

In this section, we discuss three possible alternatives to the Major Cycle: Separating the Major Cycle in two Optimization problems, using Spherical Harmonics,



For new architectures, we need to handle the $w$ term and self calibration




\subsection{Separating the Major Cycle into two Optimization Problems}
The major cycle reduces two errors simultaneously. The error introduced by the non-uniform FFT approximation, and the error introduced by the deconvolution approximation. In a sense, the major cycle is a simple optimization algorithm: It iteratively calls the non-uniform FFT and deconvolution approximations with the error from the last iteration, until the error is below a certain threshold.

\begin{alignat}{1}
FT:\: \underset{I_{dirty}}{minimize} \:& \left \|  V - AI_{dirty} \right \|_2^2\label{killmajor:sep:major}\\
deconvolution:\: \underset{x}{minimize} \:& \left \| I_{dirty} - x \star PSF \right \|_2^2 + \lambda \left \| P(x) \right \|_1 \label{killmajor:sep:deconv}
\end{alignat}

The major cycle can also be separated into two objectives \eqref{killmajor:sep:major} and \eqref{killmajor:sep:deconv}, and solved in sequence. The first objective \eqref{killmajor:sep:major} tries to find the best uniformly spaced image $I_{dirty}$ matching the observation. $A^{-1}$ represents the non-uniform inverse FFT approximation ($A^{-1}I_{dirty} \approx F^{-1}I_{dirty}$). When this is solved, the original Visibility measurements are not needed any more. Then, the second objective \eqref{killmajor:sep:deconv} searches for the deconvolved image $x$ regularized by some prior $P()$.

By separating the two objectives, the hope is we can use specialized optimization algorithms that overall converge faster than several major cycles. Also, after the first objective was minimized, we can drop the Visibilities, which can be magnitudes larger than the image. 

At first glance, this is a sensible idea, but the devil is in the objective \eqref{killmajor:sep:deconv}. Namely in the $PSF$ is not constant, it varies over the image. In the Major Cycle Architecture, a constant $PSF$\footnote{CLEAN implementations in Radio Astronomy typically use a constant $PSF$. This is not strictly necessary, one can also implement CLEAN with a varying $PSF$.} is sufficient: It only needs to approximate the deconvolution. The error introduced by a constant $PSF$ can be reduced in the next Major Cycle. By eliminating the Major Cycle, we also eliminated the chance to approximate the deconvolution with a constant $PSF$. In the worst case, we would need to estimate the $PSF$ for each pixel in the image.

In the worst case, these many $PSF$s may stop us from using this approach. It is plausible that can use fewer $PSF$s and be close enough. No known work done in the radio astronomy. The Major Cycle is ubiquitous. But it is plausible.

However, the problem is that the imaging algorithm should also be able to calibrate. With this approach, there is a strict one way flow from measurements to image. For calibration, a backwards flow from image to measurements is necessary.

For calibrated data, this approach may be potentially faster. but since MeerKAT will be difficult to calibrate correctly, this is unlikely.

Fourier Transform for PSF

\subsection{Reconstruction on the Sphere}
The signal naturally lives on the celestial sphere.

Not that much used in Radio Astronomy reconstructions. Work that uses to project on the sphere and looks at the reconstruction quality \cite{mcewen2011compressed}.

The wide field of view measurement equation \eqref{meerkat:ftsphere} can be expanded into spherical harmonics.

\cite{carozzi2015imaging} measurement equation with spherical harmonics. Essentially replaces the non-uniform FFT from the Major cycle with a new approach based on the fact that it has a solution to the Helmholtz equation.
, see \cite{carozzi2015imaging} for further detail. It can be used as a different way to arrive at a dirty image from wide field of view interferometers. It is useful in full sky image, because it fulfills the (property from paper) and the full sky dirty image does not have artifacts. It does not gain any complexity advantage, quite the contrary.

Even if the spherical harmonics transform would gain a speed advantage, used in the way of \cite{carozzi2015imaging} it would still be the same major cycle architecture, just with spherical harmonics transform.

However there is the possibility to just do everything on the sphere.

Spherical haar wavelets used for simulating visibilities from a known image, essentially the opposite operation from imaging. \cite{mcewen2008simulating}

based on the spherical harmonic transform. Doing a Spherical harmonic transform efficiently is still an active field of research \cite{schaeffer2013efficient}

transform
. The wide field of view measurement equation \eqref{meerkat:ftsphere} can be factorized into spherical harmonics. This means spherical harmonics are equivalent, calculating an image from the measurements with the three dimensional fourier transform is the same as using the spherical harmonics transform. \cite{shaw2014all}

\cite{mcewen2013sparse}sampling theorem on a sphere



Spherical harmonics were researched in the context of Compressed Sensing image reconstructions (cite wiaux). They improved the quality of the image reconstruction, but no speed advantage.

The push on solving on the sphere directly. Using spherical haar wavelets, (cite) was able to speed up the simulation problem. Simulation tries to solve to problem of finding the measurements corresponding to a given image $x$. However, this has so far not been used for imaging.


\subsection{Direct Fourier Transform}
Hardy et al\cite{hardy2013direct} handled the whole inverse Fourier Transform as an explicit matrix $F^{-1}$ of size  $M*N$. They dropped the non-uniform FFT and $w$-stacking approximations. This is trivially to distribute later. The only problem is of Course that for MeerKAT data sizes $M*N$ is too large (4gb of visibilities times $1$ Billion Pixels).

However there is potential for improvement: Only a limited number of pixels in a reconstruction are not zero. We spent a lot of processing power on pixels which do not matter in the reconstruction. If we could predict which pixels are not zero, we would only need to calculate a subset of $F^{-1}$ columns to reconstruct the image. The starlet transform\cite{starck2015starlet} has a way to predict its non-zero components. We represent the image as a combination of starlets, and estimate which starlets are likely non-zero from the Visibilities directly.

To my knowledge, such an algorithm has never been tried out.

A proof-of-concept algorithm was developed that shows the prediction actually works in practice. It uses Coordinate Descent with the starlet transform, and only needs to calculate a subset of the matrix $F^{-1}$ for a reconstruction. The question remains how many columns of $F^{-1}$ are needed, and is it more efficient than an algorithm using the major cycle architecture.
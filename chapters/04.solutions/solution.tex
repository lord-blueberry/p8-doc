\section{Eliminating the Major Cycle}\label{killmajor}
Although Compressed Sensing algorithms produce higher quality reconstructions, the CLEAN algorithm so far has lower runtime costs. Both use a version of the Major Cycle architecture for efficient transformation of Visibilities to image space, and to correct the effects of wide field of view imaging. This project explores different architectures for Compressed Sensing, with the goal to decrease the runtime costs in the context of MeerKAT.

Many Compressed Sensing reconstructions use the Major Cycle architecture\cite{girard2015sparse, dabbech2018cygnus, mcewen2011compressed, pratley2018fast}  and do not discuss alternatives. Runtime improvements are explored in the context of the Major Cycle. Pratley et al\cite{pratley2017robust} optimized the non-uniform FFT in the context of Compressed Sensing reconstructions. Dabbech et al.\cite{dabbech2017wEffect} investigated $w$-term approximations for reducing runtime costs of Compressed Sensing reconstructions. 

An alternative to the Major Cycle was investigated by Hardy et al.\cite{hardy2013direct}. They replaced the non-uniform FFT with the direct Fourier transform and distributed the problem.

We

We want explore further and discuss. why they don't work.
In this section, we discuss three possible alternatives to the Major Cycle: Separating the Major Cycle in two Optimization problems, using Spherical Harmonics,



\subsection{Separating the Major Cycle into two Optimization Problems}
In each iteration, the Major Cycle reduces the error from both, the non-uniform FFT approximation, and from incomplete measurements. Here, we discuss the potentials and problems of separating the problem into two optimization objectives \eqref{killmajor:sep:major} \eqref{killmajor:sep:deconv} and solve each after the other. This architecture only needs the dirty image for further processing, which is several times smaller than the Visibility measurements.

\begin{alignat}{1}
\underset{I_{dirty}}{minimize} \:& \left \|  V - \tilde{F}I_{dirty} \right \|_2^2\label{killmajor:sep:major}\\
\underset{x}{minimize} \:& \left \| I_{dirty} - x \star PSF \right \|_2^2 + \lambda \left \| P(x) \right \|_1 \label{killmajor:sep:deconv}
\end{alignat}

The first objective \eqref{killmajor:sep:major} uses the non-uniform FFT approximation $\tilde{F}$ and searches for the optimal dirty image matching the Visibilities. The optimization algorithm for the first objective looks similar to the Major Cycle architecture. It still uses the non-uniform FFT approximation, and iteratively transforms the Visibilities to image space and back. In this architecture however, we can use specialized optimization algorithms potentially uses fewer non-uniform FFT approximations to converge to the dirty image. At this point, we can drop the original Visibility measurements and only use the dirty image for further processing. We reconstruct the observed image $x$ by minimizing a the second objective \eqref{killmajor:sep:deconv}, which just needs the dirty image and the $PSF$.

The downside of this architecture lies in the $PSF$ of the second objective. The $PSF$ varies over the image. For the most accurate result, we would need a $PSF$ for each pixel. A single $PSF$ is calculated from our Visibilities, where all amplitudes are 1 and phase-shifted to the desired position. For every $PSF$ calculation we need another non-uniform FFT approximation, leading to a quadratic runtime.

CLEAN gets away with a constant $PSF$\footnote{CLEAN implementations in Radio Astronomy typically use a constant $PSF$. This is not necessary, one can also implement CLEAN with a varying $PSF$.}, because of the Major Cycle architecture. In each cycle, it can reduce the error it introduced in the previous cycle, leading to a more accurate reconstruction. In the new architecture, we need several $PSF$s for a comparable accuracy to the Major Cycle. 



For calibrated data, this approach may be potentially faster. but since MeerKAT will be difficult to calibrate correctly, this is unlikely.

\subsection{Spherical Harmonics}
Spherical Harmonics are a different way to represent the measurements of an interferometer. Carozzi\cite{carozzi2015imaging} derived a measurement equation based on the fact that the Visibilities fulfils the Helmholtz equation. In practical terms, Carozzi showed we can replace the non-uniform FFT with the Spherical Harmonics Transform in the Major Cycle architecture.

Deconvolution algorithms like CLEAN are still possible, but the true potential is reconstructing on the sphere directly. MCewen et al.\cite{mcewen2008simulating} showed how to do wavelets on the sphere and used it to speed up the simulation of Visibilities.

This is the only work in that area. The rest is focussed on improving the reconstruction quality instead of speed.\cite{mcewen2011compressed}

It is unknown, maybe there is a way to improve the runtime, but so far nobody has published anything.

If there is, it may be difficult to translate calibration, ionosphere and the stuff to the spherical harmonics measurement equation. 

We have had decades of Fourier-related experience and what effects they do. RIME


\subsection{Direct Fourier Transform improvement}


Hardy et al\cite{hardy2013direct} handled the whole inverse Fourier Transform as an explicit matrix $F^{-1}$ of size  $M*N$. They dropped the non-uniform FFT and $w$-stacking approximations. This is trivially to distribute later. The only problem is of Course that for MeerKAT data sizes $M*N$ is too large ().

However there is potential for improvement: Only a limited number of pixels in a reconstruction are not zero. We spent a lot of processing power on pixels which do not matter in the reconstruction. If we could predict which pixels are not zero, we would only need to calculate a subset of $F^{-1}$ columns to reconstruct the image. The starlet transform\cite{starck2015starlet} has a way to predict its non-zero components. We represent the image as a combination of starlets, and estimate which starlets are likely non-zero from the Visibilities directly.

To my knowledge, such an algorithm has never been tried out.

A proof-of-concept algorithm was developed that shows the prediction actually works in practice. It uses Coordinate Descent with the starlet transform, and only needs to calculate a subset of the matrix $F^{-1}$ for a reconstruction. The question remains how many columns of $F^{-1}$ are needed, and is it more efficient than an algorithm using the major cycle architecture.
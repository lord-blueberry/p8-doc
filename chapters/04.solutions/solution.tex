\section{Eliminating the Major Cycle}\label{killmajor}
There is little research in exploring a different architecture for Compressed Sensing. The closest to this work was done by Pratley et al\cite{pratley2017robust}, which optimizes the non-uniform FFT in the context of compressed sensing. The Major cycle is still used. On the upside, progress in w-term accuracy and runtime. 

There are ways to form a different architecture. However, the advent of self-calibration pushes a forward-backward kind of architecture on the reconstruction. 


\subsection{Separating the Major Cycle into two Optimization Problems}
The major cycle reduces two errors simultaneously. The error introduced by the non-uniform FFT approximation, and the error introduced by the deconvolution approximation. In a sense, the major cycle is a simple optimization algorithm: It iteratively calls the non-uniform FFT and deconvolution approximations with the error from the last iteration, until the error is below a certain threshold.

\begin{alignat}{1}
FT:\: \underset{I_{dirty}}{minimize} \:& \left \|  V - A^{-1}I_{dirty} \right \|_2^2\label{killmajor:sep:major}\\
deconvolution:\: \underset{x}{minimize} \:& \left \| I_{dirty} - x \star PSF \right \|_2^2 + \lambda \left \| P(x) \right \|_1 \label{killmajor:sep:deconv}
\end{alignat}

The major cycle can also be separated into two objectives \eqref{killmajor:sep:major} and \eqref{killmajor:sep:deconv}, and solved in sequence. The first objective \eqref{killmajor:sep:major} tries to find the best uniformly spaced image $I_{dirty}$ matching the observation. $A^{-1}$ represents the non-uniform inverse FFT approximation ($A^{-1}I_{dirty} \approx F^{-1}I_{dirty}$). When this is solved, the original Visibility measurements are not needed any more. Then, the second objective \eqref{killmajor:sep:deconv} searches for the deconvolved image $x$ regularized by some prior $P()$.

By separating the two objectives, the hope is we can use specialized optimization algorithms that overall converge faster than several major cycles. Also, after the first objective was minimized, we can drop the Visibilities, which can be magnitudes larger than the image. 

At first glance, this is a sensible idea, but the devil is in the objective \eqref{killmajor:sep:deconv}. Namely in the $PSF$ is not constant, it varies over the image. In the Major Cycle Architecture, a constant $PSF$\footnote{CLEAN implementations in Radio Astronomy typically use a constant $PSF$. This is not strictly necessary, one can also implement CLEAN with a varying $PSF$.} is sufficient: It only needs to approximate the deconvolution. The error introduced by a constant $PSF$ can be reduced in the next Major Cycle. By eliminating the Major Cycle, we also eliminated the chance to approximate the deconvolution with a constant $PSF$. In the worst case, we would need to estimate the $PSF$ for each pixel in the image.

In the worst case, these many $PSF$s may stop us from using this approach. It is plausible that can use fewer $PSF$s and be close enough. No known work done in the radio astronomy. The Major Cycle is ubiquitous. But it is plausible.

However, the problem is that the imaging algorithm should also be able to calibrate. With this approach, there is a strict one way flow from measurements to image. For calibration, a backwards flow from image to measurements is necessary.

For calibrated data, this approach may be potentially faster. but since MeerKAT will be difficult to calibrate correctly, this is unlikely.


\subsection{Reconstruction on the Sphere}
The signal naturally lives on the celestial sphere.

Not that much used in Radio Astronomy reconstructions. Work that uses to project on the sphere and looks at the reconstruction quality \cite{mcewen2011compressed}.

The wide field of view measurement equation \eqref{meerkat:ftsphere} can be expanded into spherical harmonics.

\cite{carozzi2015imaging} measurement equation with spherical harmonics. Essentially replaces the non-uniform FFT from the Major cycle with a new approach based on the fact that it has a solution to the Helmholtz equation.
, see \cite{carozzi2015imaging} for further detail. It can be used as a different way to arrive at a dirty image from wide field of view interferometers. It is useful in full sky image, because it fulfills the (property from paper) and the full sky dirty image does not have artifacts. It does not gain any complexity advantage, quite the contrary.

Even if the spherical harmonics transform would gain a speed advantage, used in the way of \cite{carozzi2015imaging} it would still be the same major cycle architecture, just with spherical harmonics transform.

However there is the possibility to just do everything on the sphere.

Spherical haar wavelets used for simulating visibilities from a known image, essentially the opposite operation from imaging. \cite{mcewen2008simulating}

based on the spherical harmonic transform. Doing a Spherical harmonic transform efficiently is still an active field of research \cite{schaeffer2013efficient}

transform
. The wide field of view measurement equation \eqref{meerkat:ftsphere} can be factorized into spherical harmonics. This means spherical harmonics are equivalent, calculating an image from the measurements with the three dimensional fourier transform is the same as using the spherical harmonics transform. \cite{shaw2014all}

\cite{mcewen2013sparse}sampling theorem on a sphere



Spherical harmonics were researched in the context of Compressed Sensing image reconstructions (cite wiaux). They improved the quality of the image reconstruction, but no speed advantage.

The push on solving on the sphere directly. Using spherical haar wavelets, (cite) was able to speed up the simulation problem. Simulation tries to solve to problem of finding the measurements corresponding to a given image $x$. However, this has so far not been used for imaging.


\subsection{Coordinate Descent}

cite : Direct deconvolution of radio synthesis images using L1 minimisation

Coordinate descent is an optimization algorithm, which can be interesting on LASSO objectives \eqref{killmajor:lasso}. The image $x = D\alpha$ and $F^{-1}$ stands for the three dimensional inverse Fourier transform. It is interesting because it does not need a major cycle, and can handle the $w$ term without any extra operation.


\begin{equation}\label{killmajor:lasso}
\underset{X}{minimize} \: \left \| V - F^{-1}X \right \|_2^2 + \lambda \left \| X \right \|_1 \\
\end{equation}

\begin{equation}
\underset{\alpha}{minimize} \: \left \| V - F^{-1}D \alpha \right \|_2^2 + \lambda \left \| \alpha \right \|_1 \\
\end{equation}

Coordinate descent minimizes the objective by descending one coordinate at a time. At a given point, all $\alpha$ are fixed except for one, which gets minimized. If the problem has the LASSO form, it forms a parabola which can be minimized analytically.

The Matrix Product $F^{-1}*D$ has the dimensionality $M*S$, where $M$ is the number of measurements and $S$ is the number of components in the dictionary. Since compressed sensing gets used with over-complete dictionaries, $S$ is larger than the number of pixels $N$. For MeerKAT scale data, the matrix product is too big to calculate explicitly.  The good news is that only small number of $\alpha$ are non-zero, so if there is a heuristic to find candidate $\alpha$'s we only need to calculate a subset of $F^{-1}*D$. It turns out, with the Starlet Basis one such heuristic exists.

The question is, can the Coordinate Descent with exact transformations out-perform the major cycle approaches, which uses approximations.



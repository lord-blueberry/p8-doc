\section{Compressed Sensing with Coordinate Descent\\ and the direct Fourier Transform}\label{cd}
Instead of using the Major Cycle architecture and approximate the non-uniform FFT approximation, we directly use the Fourier transform matrix $F$. We show the principle of our approach and of Compressed Sensing reconstructions in general at a simplified example. Let us minimize the objective function \eqref{cd:clean}.

\begin{equation}\label{cd:clean}
\underset{x}{minimize} \: \left \| V - Fx \right \|_2^2 + \lambda \left \| x \right \|_1 \\
\end{equation}

The data term $\left \| V - F^{-1}x \right \|_2^2$ forces our image to be similar to the measurements, and the regularization term $\left \| x \right \|_1$ tells us how likely the current reconstruction is to be the true image. The parameter $\lambda$ weights the trade-off between the data and the regularization term. With our term we assume our image contains only a limited number of non-zero pixels.  When the instrument observes stars, they take up a single pixel in the image. In this case, our prior models the true image well, and the theory of compressed sensing states we are virtually guaranteed to find the truly observed image at the minimum of our objective function \eqref{cd:clean}.

We can use a number of different optimization algorithms to optimize \eqref{cd:clean}. Note however in the one dimensional case, where we only have one pixel, the data term of the objective \eqref{cd:clean} forms a parabola and the regularization term a shrink operation\footnote{The shrink operation reduces the magnitude of the pixel by $\lambda$. For example: Pixel $x = -12.4$, $\lambda = 0.6$. The new pixel value after shrinkage follows as $shrink(-12.4, 0.6) = -11.9$}. The optimum for a single pixel can be calculated by solving for the minimum of the parabola first, followed by a shrink operation. With Coordinate Descent we can exploit this property. We fix all pixels except for one and iteratively solve for the current minimum of each pixel. Now when we iterate over all pixels several times, we eventually converge to a solution. The full Coordinate Descent algorithm can be implemented in a few lines of python code:

\begin{lstlisting} 
def coordinate_descent(V_residual, x, lambda, max_iter):
	for k in range(0, max_iter):
		for i in pixels_row:
			for j in pixels_column:
				x_old = x[i, j]
				fourier_column = calculate_fourier_transform(i, j)
				fr = real(fourier_column)
				fi = imag(fourier_column)
				rr = real(V_residual)
				ri = imag(V_residual)
				
				#find apex
				a = sum(fr**2 + 2*fr*fi + fi**2)
				b = sum(fr*rr + fr*ri + fi*rr + fi*ri)
				x_new = b / a + x_old
				
				x_new = shrink(x_new, lambda)
				x[i, j] = x_new
				V_residual = V_residual - fourier_column * (x_new - x_old)
\end{lstlisting}\label{cd:basic}

Coordinate Descent has to iterate over every pixels possibly several times. How quickly Coordinate Descent converges in theory is not well understood. Our optimization function \eqref{cd:clean} falls in the class of quadratic programming with a strictly convex objective. In this case, Coordinate Descent is guaranteed to converge at least linearly\cite{luo1992convergence}. Sadly, real-world objective functions for image reconstruction are often not strictly convex. Usually our image is constrained to have only positive pixels\cite{mcewen2011compressed}, which breaks the strictly convex property of the objective function. In our environment, the convergence guarantees of Coordinate Descent are not well understood in theory.

In practice, Coordinate Descent can be improved with heuristics. Since we assume our image contains only a few stars, a few non-zero pixels, the simple Coordinate Descent algorithm wastes resources checking if the pixel is still zero. A better scheme would be the active set heuristic\cite{friedman2010regularization}: We iterate over every pixel once. Then, for a given number of iterations, we only check the pixels that have changed. This is an improvement, but too expensive in the context of MeerKAT. A reconstruction problem can have billions of Visibilities and serveral millions of pixels. Since $F$ has the size of Visibilities times pixels, it gets too expensive to calculate all columns even once.

However, it turns out we do not need to if we imitate CLEAN: For CLEAN, the non-uniform FFT approximates the 'dirty' image, which is corrupted by the effects of incomplete measurements. CLEAN then subtracts a fraction of the PSF at the largest pixel value. In other words, the largest pixel value in the dirty image is the most likely non-zero pixel. We can use the dirty image approximation for a 'probability distribution' of non-zero pixels. It does not need to be accurate, since the actual reconstruction is done with the direct Fourier Transform.

This is, in principle, how the proof-of-concept algorithm works. Instead of reconstructing an image, it reconstructs in the starlet space. It uses the starlet transform of the dirty image to find likely non-zero components. Coordinate Descent is used to optimize single starlets. A proof-of-concept version of this algorithm was developed in python.

Although the algorithm produces super-resolved images in section \ref{results}, it is currently not known if it actually converges to the true optimum. Finding all relevant non-zero starlets is left to a greedy heuristic. There may be conditions under which it never includes relevant components. Further convergence analysis was outside the scope of the project. Also note that for this implementation, every time likely non-zero components are searched, it simply uses the non-uniform FFT for the dirty image approximation. This was done for simplicity's sake and can be improved.


\subsection{The Starlet Transform} \label{cd:starlets}
First we introduce the starlet transform, and then describe how we use it to estimate likely non-zero starlet components.

\begin{figure}[h]
	\centering
	\begin{subfigure}[b]{0.4\linewidth}
		\includegraphics[width=\linewidth]{./chapters/05.algorithms/starlets/scaling.png}
		\caption{Spline scaling function}
		\label{cd:starlets:scaling}
	\end{subfigure}
	\begin{subfigure}[b]{0.4\linewidth}
		\includegraphics[width=\linewidth]{./chapters/05.algorithms/starlets/wavelet.png}
		\caption{Wavelet function}
		\label{cd:starlets:wavelet}
	\end{subfigure}
	\caption{Scaling and wavelet function of the starlet wavelet. Source: \cite{starck2015starlet}}
	\label{cd:starlets:figure}
\end{figure}

The starlet transform is an Isotropic Undecimated Wavelet Transform (IUWT) with the starlet wavelet shown in figure \ref{cd:starlets:figure}. It defines two operations: The transformation from starlet into image space which is called synthesis, and the inverse which is called decomposition. The starlet space represents an image in multiple layers at different scales, where lower layers contain smaller objects and upper layers the extended emissions. The lowest starlet layer represents the stars, while the upper layer represents the widest hydrogen clouds in the image.

\begin{equation}\label{cd:starlet:w0}
\begin{aligned}
	c_0 &= \bm{x} \star B_0 \\
	\bm{w_0} &= \bm{x} - c_0
\end{aligned}
\end{equation}

Let us look at the lowest starlet layer first, how it is calculated from the image, and how we use it for image reconstruction. The starlet layer $w_0$ gets calculated in two steps, shown in \eqref{cd:starlet:w0}. First, we take the scaling function for the lowest layer $B_0$, and convolve the image with it, resulting in the blurred image $c_0$. The blurring removes the smaller structures of the image. In the last step, we subtract the blurred image $c_0$ from $x$ and arrive at the starlet layer 0, $w_0$. It contains the smallest structure of the image convolved with the starlet wavelet. 

This is where the starlet transform ends. Note that we do not get the actual starlet components $\alpha_0$ for the layer. The transformation just decomposes the structures of the image into layers. The actual starlet components $\alpha_0$ would be the result of a deconvolution. 

\begin{figure}[h]
	\centering
	\begin{subfigure}[b]{0.25\linewidth}
		\includegraphics[width=\linewidth]{./chapters/05.algorithms/starlets/dirty2.png}
		\caption{Dirty Image with two point sources.}
		\label{cd:heuristic:dirty}
	\end{subfigure}
	\begin{subfigure}[b]{0.25\linewidth}
		\includegraphics[width=\linewidth]{./chapters/05.algorithms/starlets/starlet0_2.png}
		\caption{Layer $w_0$ of the starlet decomposition.}
		\label{cd:heuristic:starlet}
	\end{subfigure}
	\caption{Dirty map and $w_0$ starlet layer after shrinkage. $w_0$ can be interpreted as a probability distribution for point source locations.}
	\label{cd:heuristic:figure}
\end{figure}

It turns out we can estimate the starlet components $\alpha_0$ by simply shrinking the starlet layer $w_0$. Figure \ref{cd:heuristic:figure} shows an example of the process. Coordinate Descent will reconstruct the image with starlet components $\alpha_0$, but it does not need to try all components. We use the starlet transform as a heuristic and can estimate likely non-zero $\alpha_0$.

The same process is repeated for all $J$ layers. We calculate the starlet decomposition shown in equation \eqref{cd:starlet:decomposition}. We continually blur the image with ever wider scaling functions $B_i$. We subtract the more heavily blurred image $ c_{J-1}$ from $c_{J-2}$ to arrive at the starlet layer, which contains the structures of the current size $w_{J-1}$. Note that the last starlet layer is the most blurred version of the image $c_J$. It contains all structures which were wider than the widest starlet.

\begin{equation}\label{cd:starlet:decomposition}
\begin{split}
c_0 &= \bm{x} \star B_0 \\
\bm{w_0} &= \bm{x} - c_0
\end{split}
\qquad
\begin{split}
c_1 &= c_0 \star B_1 \\
\bm{w_1} &= c_0 - c_1
\end{split}
\qquad \ldots \qquad
\begin{split}
c_{J-1} &= c_{J-2} \star B_{J-1} \\
\bm{w_{J-1}} &= c_{J-2} - c_{J-1}
\end{split}
\qquad
\begin{split}
\bm{c_J} &= c_{J-1} \star B_J
\end{split}
\end{equation}

We shrink all the starlet layers $w_i$ and $c_J$, and arrive at which components $\alpha_i$ are likely non-zero. Coordinate Descent converges on the optimal $\alpha_i$ for all layers. At this point, we can reconstruct the image by reversing the process. We convolve $\alpha_i$ with the starlet wavelet at scale i, which gives us $w_i$. Then, we sum them up to the final image \eqref{cd:starlet:backwards}.

\begin{equation} \label{cd:starlet:backwards}
\bm{x} = \bm{w_0} + \bm{w_1} + \ldots + \bm{w_{J-1}} + \bm{c_J}
\end{equation}

This is how we  leverage the starlet transform for our Coordinate Descent algorithm. Note that we can use the starlet transform as a heuristic without necessarily using the starlet components $\alpha_i$ for reconstructions. The starlet transform gives us likely locations of Gaussian-shaped emissions at different scales. We can for example use a Gaussian mixture model, and just use the starlets for likely non-zero locations. 

In this work, we do not use the wavelet shown in image \ref{cd:starlets:wavelet}. Instead, we modified the starlet to an extend where it arguably is not a wavelet any more. Still, the starlet transform with the correct wavelet as a heuristic, but reconstructed the image with a modified starlet to satisfy the non-negativity constraint. 


\subsubsection{Starlet Wavelet Modification}
Reconstruction algorithms for Radio Astronomy typically constrain the image to be non-negative\cite{mcewen2011compressed}. We can constrain the starlet components $\alpha_i$ to be non-negative, but the starlet wavelet has negative sections. Which leads to negative areas in the layers $w_0$, and negative pixel values in the reconstructed image $x$.

The simple fix is to either set all negative pixels of $x$ to zero, or to set negative values of $w_i$ to zero. Both lead to slightly different artefacts in the $x$, and both do not lead to significantly better results.

For reconstruction, we modify the starlet wavelet and set all negative sections to zero. At this point, the reconstruction starlet is arguably not a wavelet any more, since its integral is larger than zero. But now the Coordinate Descent algorithm can reconstruct a non-negative image which has led to a more sparse representation of images in the next section.


\subsection{Coordinate Descent Implementation}
We have the starlet regularization, let us put it into an algorithm. Start with the 

\begin{equation}\label{cd:starlet}
\underset{\alpha}{minimize} \: \left \| V - FD\alpha \right \|_2^2 + \lambda \left \| \alpha \right \|_1 \\
\end{equation}

The last question is how many columns we need of the Fourier Transform matrix $F$. Note that the relationship from starlet component $\alpha_i$ to image is a series of convolutions. We just need the Fourier transform for pixels, which are in the center of the non-zero starlet. Since a convolution in image space is a multiplication in the Fourier domain, we can 
If we know the non-zero starlet components $\alpha_i$, 


Proof of concept implementation. There are many ways to improve the efficiency and the convergence speed. It is here to show  the general if it is possible of the approach and if we can truly only calculate a subset of $F^{-1}$ columns.

The objective function \eqref{cd:starlet} minimizes the starlet components $\alpha$. This objective leads again to a parabola in the one dimensional case, and we can analytically find the optimum for a single component when all others are fixed. 



There are still a few problems to solve, like how to handle the matrix product $F^{-1}D$, 
With the active set heuristic, we iterate over a limited number of starlet components at a time. 

The question remains, how the matrix product $F^{-1}D$ can be calculated efficiently. Since starlet transform is just made up of a couple of convolutions, we do not have to calculate the product $F^{-1}D$ explicitly. Convolutions in image space are multiplications in Fourier space. 
So for any starlet layer $w_i$ or $c_J$, we can pre-calculate a transform vector. 
$w_2 = F^{-1}V * (\hat{S_0}\hat{S_1} - \hat{S_0}\hat{S_1}\hat{S_2})$

How many layers are needed. starlets rise in pixels with $2^J$. Meaning if we want starlets over the whole image, the number of starlets rise logarithmically to the image dimensions.

How to iterate over the image in detail.



\subsection{Iteration scheme}
Many ways to iterate over the image. A simple scheme was used here. 

Active set heuristic, find the starlets which are 

\begin{lstlisting} 
def coordinate_descent(V_residual, x, lambda, max_iter):
	for k in range(0, max_iter):
		for i in pixels_row:
			for j in pixels_column:
				x_old = x[i, j]
				fourier_column = calculate_fourier_transform(i, j)
				fr = real(fourier_column)
				fi = imag(fourier_column)
				rr = real(V_residual)
				ri = imag(V_residual)
				
				#find apex
				a = sum(fr**2 + 2*fr*fi + fi**2)
				b = sum(fr*rr + fr*ri + fi*rr + fi*ri)
				x_new = b / a + x_old
				
				x_new = shrink(x_new, lambda)
				x[i, j] = x_new
				V_residual = V_residual - fourier_column * (x_new - x_old)
\end{lstlisting}\label{cd:implementation}

coordinate descent with active set heuristic


sub-zero pixels

\subsubsection{Non-uniform FFT approximations}
Non uniform FFT to calculate the convolution KERNELS!!


Stupid approach with line search. Could be done more efficiently by using the histogram of the starlet level.


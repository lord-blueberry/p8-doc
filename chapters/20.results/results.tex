\section{Results and scalability of Coordinate Descent}
Two Tasks: Show that coordinate descent works, and show its scalability. To show that it works, it was tested on two simulated MeerKAT data sets and compared to clean. The simulation was created with the Meqtrees Software package, using default settings. The simulations contain [only little gaussian noise on the Visibiliites]. The simulated data is therefore perfectly calibrated, the image reconstruction has to deal with the two fundamental issues of incomplete measurements and non-uniform sampling only.

Scalability is harder to analyse for Coordinate Descent. As mentioned in previous chapters, it does not have fast convergence guarantees. But with the heuristics  but it is ripe for heuristics that work well for the special case. Therefore it was analysed what an ideal Coordinate Descent implementation would need in terms of complexity.

Two simulations, one of point sources and one of mixed sources. 

\subsection{Imaging on Simulated Data}

$\lambda$ was fixed on both tests

\begin{figure}[h]
	\centering
	\begin{subfigure}[b]{0.45\linewidth}
		\includegraphics[width=\linewidth]{./chapters/05.algorithms/sim02/sim02_point_dirty.png}
		\caption{Dirty Image of two point sources}
		\label{results:point:dirty}
	\end{subfigure}
	\begin{subfigure}[b]{0.45\linewidth}
		\includegraphics[width=\linewidth]{./chapters/05.algorithms/sim02/image4.png}
		\caption{Reconstruction after 4 Full iterations}
		\label{results:point:cd}
	\end{subfigure}
	\caption{Image reconstruction on two point sources.}
	\label{results:point}
\end{figure}

faint source is spread more widely. Not good, The paper \cite{starck2015starlet} used different $\lambda$ values for different starlet levels. Maybe with this it can be forced to do more precise results, but it was not tried in this work.

\subsubsection{Mixed Sources}
Mixture of gaussian and point sources. 

\begin{figure}[h]
	\centering
	\includegraphics[width=0.5\linewidth]{./chapters/05.algorithms/results/sim00_mixed_sources_dirty.png}
	\caption{Dirty Image}
	\label{alg:gauss:dirty}
\end{figure}

\begin{figure}[h]
	\centering
	\begin{subfigure}[b]{0.45\linewidth}
		\includegraphics[width=\linewidth]{./chapters/05.algorithms/results/image.png}
		\caption{Reconstruction after one full iteration}
		\label{results:g55:nrao:rec}
	\end{subfigure}
	\begin{subfigure}[b]{0.45\linewidth}
		\includegraphics[width=\linewidth]{./chapters/05.algorithms/results/image4.png}
		\caption{Reconstruction after 4 Full iterations}
		\label{results:g55:nrao:dirty}
	\end{subfigure}
	\caption{}
	\label{results:g55:nrao}
\end{figure}

\subsection{Scalability Comparison}

Number of Visibilities $M$

Number of Pixels $N$

\subsubsection{Lower Bound on Coordinate Descent}
How many pixels need to be evaluated.

Bunch of heuristics. There are a lot of little ways to optimize this algorithm. The question is, is it worth going further and try to improve the algorithm further. So we want to estimate the lower bound of the algorithm.

Convergence is hard to determine in general. Following simplified assumptions were made: We have a heuristic with oracle performance. It returns only the locations of $\alpha$ in constant time. Furthermore, we assume all axes are independent from each other, only one descent per non-zero axis is necessary.
$S$

$res * starlet = M$
descent:
$gen fcol = 3*M$
$a = 3 * M$
$b = 4 * M$
$residuals, fcol*diff =  M$

$total_mit_gencol = 11M$

\begin{equation}\label{results:cd:omega}
	CD(M, S, J) = S * 11M + J * 2M
\end{equation}

$S$ depends on the image content directly. For example if the image contains 15 point sources and five Gaussian extended emissions, then $S$ equals 20 non-zero components (if we assume the Gaussian sources require only one starlet for representation). Coordinate Descent therefore is independent of the image size $N$. It solely depends on the size of the measurements $M$, and the number of non-zero components in the dictionary $S$. 

It does not use any approximation for the fourier transform.

Assumptions for $J$ = 12

\subsubsection{CLEAN average case}

\begin{alignat*}{1}
	\text{non-uniform FFT} &: M + 2N*ld(2N)\\
	w \text{-stacking} &:M + W*(2N*ld(2N) + 2N) + N*ld(N)\\
	\text{CLEAN deconvolution} &: D*2N
\end{alignat*}

(Cite New W-stacking approach)
Nufft: $M + 2N log 2N$
W-stacking = $M + W*(2N log 2N + 2N) + N log N$
Deconvolution = $D*2N$


\begin{equation}
CLEAN(M, M_{Major}, N,  W, D) = M_{Major} * [M + W*(2N log 2N + 2N) + N log N + D*2N]
\end{equation}

Complex formula. But for MeerKAT, the number of Visibilities $M$ is by far the largest. Simplifies to $M_{Major}*M$

Assumptions
Estimates for $M_{Major}$ = 10. (Cite New W-stacking approach)
W = $100$ depends on observation between 10, and hundreds. Depends on the observation. Cite new w-stacking approach can limit the number of w-stacks. 
D=300'000

\subsubsection{Comparison on MeerKAT data}
Target, what would an Ideal Compressed Sensing implementation do.

MeerKAT has by far more Visibilities than pixels

Write one meerkat observation

The number of $S$ and then the lower bound of Coordinate Descent. (Put WSCLEAN image in here and Discuss $S$)

Other compressed sensing approaches have a more expensive term than $D*2N$ for their complexity. But compared to compressed sensin.



\subsection{evaluation}


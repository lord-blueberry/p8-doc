\section{Runtime cost comparison on a real-world MeerKAT reconstruction}\label{scale}
Current Compressed Sensing reconstructions produce images at a higher quality than CLEAN. However, CLEAN is significantly cheaper to compute. MeerKAT's large scale reconstruction problems, CLEAN is still the go-to algorithm. In this project, we developed for a new architecture, which uses the relevant columns of the Fourier Transform Matrix directly. We developed a Coordinate Descent algorithm with this architecture and demonstrated in section \ref{results} super-resolution performance on simulated data. The question, if we can lower the runtime costs with our new architecture, is still open. 

In this section, we compare the costs of Coordinate Descent with WSCLEAN, which is the reconstruction algorithm of choice for MeerKAT reconstructions. We create cost functions for each algorithm, which estimate the number of operations depending on the input size. WSCLEAN was executed on a real-world MeerKAT dataset shown in image \ref{scale:wsclean}. Our proof-of-concept implementation was not able to handle the large amount of data. Instead, we extrapolate the best-case costs of our approach and compare them to WSCLEAN on the MeerKAT dataset.


\subsection{Cost function of an idealized Coordinate Descent}
The runtime cost of Coordinate Descent depends on the number of Visibilities $M$ and the number of non-zero starlets $S$. The number and location of the $S$ non-zero starlets are generally not known. However, we created a heuristic which finds likely non-zero starlet components. In a realistic setting, the heuristic will have found more than $S$ likely non-zero starlets. For the idealized version of Coordinate Descent, we assume an oracle performance heuristic: It finds the location and number of the $S$ non-zero starlet components in constant time. Coordinate Descent therefore has to calculate the value of $S$  components. In total, the idealized Coordinate Descent algorithm uses four steps: creating $J$ starlet levels with the non-uniform FFT, creating the columns of $F^{-1}$, calculating the minima for each single component, and calculating the starlet layers:

\begin{alignat*}{1}
J\: \text{non-uniform FFTs for the starlet regularization} &: J*(M + 2N*ld(2N))\\
\text{creating} \:S\: \text{columns of}\: F &: S*7M\\
\text{locating} \:S\: \text{minima of} \:S\: \text{parabolas} &: S*4M\\
\text{calculating} \:J\: \text{Starlet layers} &: J * 2M
\end{alignat*}

We assume we have enough memory to cache the columns of $F^{-1}$ and only need to calculate them once. Keep in mind that each column of $F$ has the same length as the Visibilities, essentially multiplying the input data. The last parameter for Coordinate Descent is the number of iterations to converge, $I_CD$. Estimating this number is difficult as Coordinate Descent does not have strict guarantees (as discussed in section \ref{cd}). Instead, we assume it converges after a fixed number of iterations. Therefore we arrive at the cost function of \eqref{results:cd:omega}.

\begin{equation}\label{results:cd:omega}
\begin{aligned}
	CD(I_{CD}, M, S, J) = &I_{CD} * [S * 4M + J * 2M]\\
		&+  S*7M\\
		&+ J*(M + 2N*ld(2N))
\end{aligned}
\end{equation}

Note that the runtime of Coordinate Descent is independent of the number of pixels. The only image related parameter in \eqref{results:cd:omega} is $J$, the number of starlet layers. The largest starlet layer represents the largest possible structure in the image, which is given by the instrument and the image resolution. The runtime only depends indirectly on the image resolution, not the total number of pixels. For simplicity, we assume the image cannot have structures larger than half the image size. For our MeerKAT example, this is more than enough to represent the largest structures.

Also note the term iterating over the $S$ non-zero starlets, $ I_{CD} * [S * 4M +\ldots]$. As it turns out, this is the Achilles heel of the algorithm. MeerKAT observations contain a very large amount of Visibilities $M$.

\subsection{Cost function of WSCLEAN}
The WSCLEAN algorithm uses the Major Cycle architecture. It uses the non-uniform FFT with $w$-stacking. The runtime costs of a single Major Cycle depends on the non-uniform FFT with $w$-stacking and the number of CLEAN deconvolutions. $N$ denotes the number of pixels.

\begin{alignat*}{1}
	\text{non-uniform FFT} &: M + 2N*ld(2N)\\
	w\text{-stacking} &:M + W*(2N*ld(2N) + 2N) + N*ld(N)\\
	I_{CLEAN}\: \text{deconvolutions} &: I_{WSCLEAN}*2N
\end{alignat*}

The overall cost function shown in \eqref{results:clean:o} can also be split into two parts. In each Major Cycle, the forward and backwards non-uniform FFTs gets calculated, and CLEAN deconvolves the image for a maximum number of $I_{CLEAN}$ deconvolutions.

\begin{equation}\label{results:clean:o}
\begin{aligned}
 WSCLEAN(I_{Major}, I_{CLEAN}, M, N,  W) =\: &I_{Major} * 2 * [M + W*(2N*ld(2N) + 2N) + N log N]\\
	&+ [I_{CLEAN}*2N]
\end{aligned}
\end{equation}

Notice that the number of CLEAN deconvolutions $I_{CLEAN}$ depends on the image content, similar the number of non-zero starlets $S$ for Coordinate Descent. Here however, it multiplies with the number of pixels instead of the number of Visibilities. In a sense, the major cycle tries to reduce the runtime complexity of handling the image content by calculating the non-uniform FFT. If the difference is large enough $N \ll M$, then the Major Cycle will end up with a smaller runtime costs.


\subsection{Comparison on a MeerKAT reconstruction problem}
Our real-world MeerKAT observation has been calibrated and averaged in frequency and time to reduce storage space. The resulting dataset contains 540 channels with 4 million Visibilities each. Due to hardware limitations, WSCLEAN was calculated on 75 channels. The reconstructed image is shown in \ref{scale:wsclean}, and the resulting parameters for our cost function are: 
\begin{itemize}
	\item Major Cycles: $I_{Major} = 6$
	\item Number of CLEAN iterations: $I_{CLEAN} = 35'000$
	\item Visibilities: $M=3.05e^8$
		\item Pixels: $N = 2048^2$
	\item $w$-stacks: $W = 32$
\end{itemize}

\begin{figure}[h]
	\centering
	\includegraphics[width=0.8\linewidth]{./chapters/21.scalability/meerkat.png}
	\caption{WSCLEAN Reconstruction of the MeerKAT observation.}
	\label{scale:wsclean}
\end{figure}

For Coordinate Descent's costs, we need an estimate for $J$, $S$ and $I_{CD}$. We set $J=8$, which lets the largest structure span over half the image, enough to capture any large scale structures. For $S$ and $I_{CD}$, we simply use the $S=250$ from our simulated reconstruction \ref{results:mixed:cd} and set $I_{CD}=1$. In this case, we under-estimate the true value of $S$ and $I_{CD}$. The image \ref{scale:wsclean} shows complex-shaped extended emissions, which likely needs a larger number of non-zero starlets for representation than the Gaussian emissions from our simulation.

When we put all values into our cost functions \eqref{results:cd:omega} and \eqref{results:clean:o}, Coordinate Descent with the direct Fourier transform arrives at 2.3 times the costs of WSCLEAN in the best case scenario. Our approach has not reduced the runtime costs compared to WSCLEAN, even with the unrealistic assumption that Coordinate Descent only needs a single iteration to converge. Starlets are an over-complete representation. Coordinate Descent needs several iterations for competing starlets to converge to the optimal solution. The cost difference rises approximately linear with $I_{CD}$. If Coordinate Descent needs 10 iterations to converge, it is 9.6 times as expensive. If it needs 100 iterations, it is 83 times as expensive. Even in the ideal case, Coordinate Descent results in higher runtime costs than WSCLEAN.

But what about non-uniform FFT Compressed Sensing algorithms? We have not developed a cost function for Compressed Sensing approaches, but we can get a very rough estimate by changing $I_{Major}$ of the WSCLEAN cost function. Pratley et al.\cite{pratley2018fast} reported 10 cycles of non-uniform FFT approximations for a Compressed Sensing reconstruction on simulated data. If we plug in 10 Major Cycles for WSCLEAN, assume $I_{CD}=1$ again, then Coordinate Descent is still roughly twice as expensive as other non-uniform FFT Compressed Sensing algorithms. Mind you these costs are only valid when we can keep all necessary columns of $F$ in memory, eating up 1.1 terabytes\footnote{If we assume 64 bit floating point accuracy for the real and complex values of the Visibilities.} in our case. If the columns have to be re-calculated on the fly, then the runtime costs increase by magnitudes.

The issue with Coordinate Descent's runtime complexity lies in the term $I_{CD} * [S * 4M +\ldots]$ of \eqref{results:cd:omega}, which scales with the "content" of the image $S$, multiplied with the Visibilities. Coordinate Descent cannot afford many iterations nor many non-zero components, because both of these numbers get multiplied together with $M$, the largest number in the problem. With the non-uniform FFT, WSCLEAN is able to get around this limitation, and scales any content dependent factors on $N$ instead of $M$. 

Indeed, the runtime of our Coordinate Descent algorithm could be improved by using the non-uniform FFT, essentially replacing $M$ with $N$ and we arrive at the term $I_{CD} * [S * 4N +\ldots]$. In this case Coordinate Descent can afford more iterations and more non-zero components for the same runtime costs. Furthermore $N$ lies on a uniformly sampled grid. We may be able to use the non-uniform FFT instead of caching columns of $F$, and reduce the memory requirement at the same time.


\subsection{Approximations as key for going large scale}
We set out to reduce the runtime costs of compressed Sensing reconstructions by replacing the non-uniform FFT approximation. Ironically we ended up with an algorithm which not only has higher runtime costs, but the costs may even be reduced by moving back to the non-uniform FFT.In its current form the direct Fourier Transform is not feasible for large scale MeerKAT reconstructions.

The direct Fourier Transform does not use any approximations. It calculates the transform up to the limit of floating-point accuracy. The non-uniform FFT on the other hand approximates the transform up to the accuracy limit of the measurements\cite{pratley2017robust}. We do not need a more accuracy for image reconstruction. The direct Fourier Transform wastes computing resources on accuracy, which does not improve the reconstruction quality. The runtime costs of direct Fourier Transform can be improved by re-introducing approximations. For a start, each entry does not need the full floating-point precision. This change alone potentially reduces both memory requirement and runtime costs. 

The untapped potential for the direct Fourier Transform lies in the redundant Visibility data. Interferometers measure many Visibilities with similar $uvw$-coordinates, which likely contain redundant information for image reconstruction. Not all Visibilities contribute the same amount of new information to the reconstruction. Currently, neither the non-uniform FFT nor our direct Fourier Transform leverages this property to its advantage. For the direct Fourier Transform, this means we may not need all Visibilities for reconstruction. Our Coordinate Descent approach demonstrated that not all pixels are necessary for a reconstruction. The next logical step is to investigate if we can reduce the number of Visibilities too.

Approximations seem to be the key for going large scale, with the Fourier Transform approximation at the core. Depending on the choice of Fourier Transform approximation, we arrive at different obstacles and opportunities. Removing redundant Visibilities does not significantly speed up the non-uniform FFT, since it scales linearly with the number of Visibilities. The direct Fourier Transform however benefits both in terms of runtime costs and memory requirement. By leveraging the redundant information in the measurements, we may arrive at a direct Fourier Transform which is more cost-effective than the non-uniform FFT.

The choice of Fourier Transform approximation cannot be investigated without a reconstruction algorithm in mind. CLEAN algorithms synergise with the non-uniform FFT. Not only does it approximate the Fourier Transform, but the cyclic nature allows CLEAN to approximate the varying Point Spread Function with a constant one. The direct Fourier Transform does not benefit CLEAN the same way it can benefit a Coordinate Descent algorithm. The flexibility of Compressed Sensing algorithms in general allows them to work with a variety of different approximations. There may be a combination of Fourier approximation which synergises with a specific Compressed Sensing algorithm for MeerKAT reconstructions.

Our Coordinate Descent algorithm used the direct Fourier Transform instead of the non-uniform FFT. Our approach has potential for easy parallel and distributed reconstructions. Coordinate Descent steps can be calculated asynchronously. The direct Fourier Transform does not need any iterative approximation algorithm and can be distributed as a matrix multiplication. However, it is still too expensive for large scale reconstructions compared to the non-uniform FFT. There is potential to improve the direct Fourier Transform by leveraging redundant Visibilities. Whether it leads to both competitive runtime costs and reconstruction quality remains to be investigated.

For large scale Compressed Sensing reconstructions, the non-uniform FFT is both readily available and well researched in the Radio Astronomy community. It seems to be the most efficient way known to approximate the Fourier Transform. There may be more efficient combinations of approximation and reconstruction algorithm for Compressed Sensing, which may lead to a highly distributable reconstruction algorithm. But replacing the non-uniform FFT alone leads to higher runtime costs. Further approximations likely need to be introduced before an alternative can compete with the non-uniform FFT.



%As it is, our proof-of-concept algorithm with the direct Fourier Transform is too expensive for large scale reconstructions. It wastes resources on a too precise transformation and redundant Visibility information. By re-introducing an approximate transform and by reducing the redundant information, we can further decrease the runtime costs of the direct Fourier Transform. Together with Coordinate Descent,  If the overall runtime costs and memory requirement can be reduced to comparable levels of the non-uniform FFT, our Coordinate Descent with the direct Fourier Transform becomes interesting for large scale distributed reconstructions.


















 